---
title: "Simplicial Embeddings in Self-Supervised Learning and Downstream Classification"
date: Apr 2023
venue: "ICLR 2023 - Oral (top 25%)"
authors:
  - name: "Samuel Lavoie"
  - name: "Christos Tsirigotis"
    highlight: true
  - name: "Max Schwarzer"
  - name: "Ankit Vani"
  - name: "Michael Noukhovitch"
  - name: "Kenji Kawaguchi"
  - name: "Aaron Courville"
keywords:
  - self-supervised learning
  - representation learning
  - robust generalization
tldr: "Constrain SSL features onto a product of simplices (SEM) to induce structured sparsity and better downstream generalization by **+4-5% ac-
curacy** in downstream image classification tasks."
links:
  paper: "https://arxiv.org/abs/2204.00616"
  code: "https://github.com/lavoiems/simplicial-embeddings"
  explainer: "/projects/simplicial-embeddings/"
images:
  teaser: "/images/projects/ssb-ssl.jpg"
  alt: "SEM architecture intervention"
featured: true
bibtex: |
  @inproceedings{lavoie2023simplicial,
    title={Simplicial Embeddings in Self-Supervised Learning and Downstream Classification},
    author={Samuel Lavoie and Christos Tsirigotis and Max Schwarzer and Ankit Vani and Michael Noukhovitch and Kenji Kawaguchi and Aaron Courville},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023}
  }

---

SEM constrains representations to a product of simplices. That structure encourages sparsity and yields more predictable downstream behavior.

**What changes?** Replace unconstrained projections with softmaxes into L simplices of V dims each; train your SSL normally. The constraint is the inductive bias.

**Why it works (intuitively).** You trade raw capacity for structured features that cluster semantically relevant factors, improving linear separability after pretraining. The ICLR 2023 result was accepted as an **Oral (top 25%)**.
