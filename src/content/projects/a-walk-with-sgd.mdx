---
title: "A Walk with SGD"
date: 2018-02-24
venue: "arXiv 2018 (ICLR 2019 submission)"
authors:
  - Chen Xing
  - Devansh Arpit
  - Christos Tsirigotis
  - Yoshua Bengio
keywords:
  - optimization
  - SGD
  - generalization
  - loss landscape
tldr: "Empirical study showing SGD typically moves above the valley floor—large LR sets the height; small batches inject exploration—explaining better generalization."
status: "preprint"
impact: {}
links:
  paper: "https://arxiv.org/abs/1802.08770"
  explainer: "/projects/a-walk-with-sgd/"
---

**Takeaway.** During most of training, SGD doesn't constantly cross “barriers”; it bounces between valley walls at a height set by the learning rate, with mini‑batch noise enabling exploration — explaining small‑batch + large‑LR generalization.  [oai_citation:11‡arXiv](https://arxiv.org/pdf/1802.08770?utm_source=chatgpt.com)

**Links.** Project: [/projects/a-walk-with-sgd/](/projects/a-walk-with-sgd/) · arXiv: 1802.08770.  [oai_citation:12‡arXiv](https://arxiv.org/pdf/1802.08770?utm_source=chatgpt.com)

### Preferred BibTeX
```bibtex
@article{xing2018walksgd,
  title   = {A Walk with SGD},
  author  = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  journal = {arXiv:1802.08770},
  year    = {2018},
  url     = {https://arxiv.org/abs/1802.08770}
}
